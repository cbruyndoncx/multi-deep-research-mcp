{
  "request_id": "resp_04dad188ec1fcd5c006942e2e5bab4819381a43db19eebc0e3",
  "provider": "openai",
  "model": "o4-mini-2025-04-16",
  "status": "queued",
  "favorites": [],
  "parameters": {
    "reasoning": {
      "summary": "auto"
    }
  },
  "available_models": [
    {
      "id": "gpt-4o-mini-tts-2025-03-20",
      "label": "gpt-4o-mini-tts-2025-03-20",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-mini-tts-2025-12-15",
      "label": "gpt-4o-mini-tts-2025-12-15",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o",
      "label": "gpt-4o",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-2024-05-13",
      "label": "gpt-4o-2024-05-13",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-mini-2024-07-18",
      "label": "gpt-4o-mini-2024-07-18",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-mini",
      "label": "gpt-4o-mini",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-2024-08-06",
      "label": "gpt-4o-2024-08-06",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-audio-preview",
      "label": "gpt-4o-audio-preview",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-realtime-preview",
      "label": "gpt-4o-realtime-preview",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-realtime-preview-2024-12-17",
      "label": "gpt-4o-realtime-preview-2024-12-17",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-audio-preview-2024-12-17",
      "label": "gpt-4o-audio-preview-2024-12-17",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-mini-realtime-preview-2024-12-17",
      "label": "gpt-4o-mini-realtime-preview-2024-12-17",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-mini-audio-preview-2024-12-17",
      "label": "gpt-4o-mini-audio-preview-2024-12-17",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "o1-2024-12-17",
      "label": "o1-2024-12-17",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "o1",
      "label": "o1",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-mini-realtime-preview",
      "label": "gpt-4o-mini-realtime-preview",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-mini-audio-preview",
      "label": "gpt-4o-mini-audio-preview",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "o3-mini",
      "label": "o3-mini",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": true,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "o3-mini-2025-01-31",
      "label": "o3-mini-2025-01-31",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-2024-11-20",
      "label": "gpt-4o-2024-11-20",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-search-preview-2025-03-11",
      "label": "gpt-4o-search-preview-2025-03-11",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-search-preview",
      "label": "gpt-4o-search-preview",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-mini-search-preview-2025-03-11",
      "label": "gpt-4o-mini-search-preview-2025-03-11",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-mini-search-preview",
      "label": "gpt-4o-mini-search-preview",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-transcribe",
      "label": "gpt-4o-transcribe",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-mini-transcribe",
      "label": "gpt-4o-mini-transcribe",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "o1-pro-2025-03-19",
      "label": "o1-pro-2025-03-19",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "o1-pro",
      "label": "o1-pro",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-mini-tts",
      "label": "gpt-4o-mini-tts",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "o3-2025-04-16",
      "label": "o3-2025-04-16",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "o4-mini-2025-04-16",
      "label": "o4-mini-2025-04-16",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "o3",
      "label": "o3",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "o4-mini",
      "label": "o4-mini",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4.1-2025-04-14",
      "label": "gpt-4.1-2025-04-14",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4.1",
      "label": "gpt-4.1",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4.1-mini-2025-04-14",
      "label": "gpt-4.1-mini-2025-04-14",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4.1-mini",
      "label": "gpt-4.1-mini",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4.1-nano-2025-04-14",
      "label": "gpt-4.1-nano-2025-04-14",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4.1-nano",
      "label": "gpt-4.1-nano",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "o3-pro",
      "label": "o3-pro",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-realtime-preview-2025-06-03",
      "label": "gpt-4o-realtime-preview-2025-06-03",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-audio-preview-2025-06-03",
      "label": "gpt-4o-audio-preview-2025-06-03",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "o3-pro-2025-06-10",
      "label": "o3-pro-2025-06-10",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "o4-mini-deep-research",
      "label": "o4-mini-deep-research",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "o3-deep-research",
      "label": "o3-deep-research",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-transcribe-diarize",
      "label": "gpt-4o-transcribe-diarize",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "o3-deep-research-2025-06-26",
      "label": "o3-deep-research-2025-06-26",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "o4-mini-deep-research-2025-06-26",
      "label": "o4-mini-deep-research-2025-06-26",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5-chat-latest",
      "label": "gpt-5-chat-latest",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5-2025-08-07",
      "label": "gpt-5-2025-08-07",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5",
      "label": "gpt-5",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": true,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5-mini-2025-08-07",
      "label": "gpt-5-mini-2025-08-07",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5-mini",
      "label": "gpt-5-mini",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": true,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5-nano-2025-08-07",
      "label": "gpt-5-nano-2025-08-07",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5-nano",
      "label": "gpt-5-nano",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": true,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5-codex",
      "label": "gpt-5-codex",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": true,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5-pro-2025-10-06",
      "label": "gpt-5-pro-2025-10-06",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5-pro",
      "label": "gpt-5-pro",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5-search-api",
      "label": "gpt-5-search-api",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5-search-api-2025-10-14",
      "label": "gpt-5-search-api-2025-10-14",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5.1-chat-latest",
      "label": "gpt-5.1-chat-latest",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5.1-2025-11-13",
      "label": "gpt-5.1-2025-11-13",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5.1",
      "label": "gpt-5.1",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5.1-codex",
      "label": "gpt-5.1-codex",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5.1-codex-mini",
      "label": "gpt-5.1-codex-mini",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5.1-codex-max",
      "label": "gpt-5.1-codex-max",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5.2-2025-12-11",
      "label": "gpt-5.2-2025-12-11",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5.2",
      "label": "gpt-5.2",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": true,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5.2-pro-2025-12-11",
      "label": "gpt-5.2-pro-2025-12-11",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5.2-pro",
      "label": "gpt-5.2-pro",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": true,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-5.2-chat-latest",
      "label": "gpt-5.2-chat-latest",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-mini-transcribe-2025-12-15",
      "label": "gpt-4o-mini-transcribe-2025-12-15",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    },
    {
      "id": "gpt-4o-mini-transcribe-2025-03-20",
      "label": "gpt-4o-mini-transcribe-2025-03-20",
      "provider": "openai",
      "description": "OpenAI reasoning model",
      "supports_background_jobs": false,
      "supports_code_interpreter": false,
      "supports_reasoning": null,
      "parameter_descriptions": {
        "temperature": "0-2. Lower values = deterministic responses.",
        "top_p": "0-1 nucleus sampling.",
        "max_output_tokens": "Maximum tokens for the response.",
        "max_tokens": "Maximum tokens in the reply.",
        "reasoning.effort": "medium/high reasoning effort.",
        "reasoning.summary": "auto/none reasoning summaries."
      },
      "has_parameter_schema": true
    }
  ],
  "message": "Research request created successfully"
}
